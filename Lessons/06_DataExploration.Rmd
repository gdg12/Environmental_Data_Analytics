---
title: "6: Data Exploration"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2019"
output: pdf_document
geometry: margin=2.54cm
fig_width: 5
fig_height: 2.5
editor_options: 
  chunk_output_type: console
---

## LESSON OBJECTIVES
1. Set up a data analysis session in RStudio
2. Import and explore datasets in R
3. Apply data exploration skills to a real-world example dataset

## OPENING DISCUSSION: WHY DO WE EXPLORE OUR DATA?

Why is data exploration our first step in analyzing a dataset? What information do we gain? How does data exploration aid in our decision-making for data analysis steps further down the pipeline?

## IMPORT DATA AND VIEW SUMMARIES

```{r}
# 1. Set up your working directory


getwd()

# 2. Load packges
library(tidyverse)

# 3. Import datasets
USGS.flow.data <- read.csv("USGS_Site02085000_Flow_Raw.csv")

View(USGS.flow.data)
# Alternate option: click on data frame in Environment tab

class(USGS.flow.data)
colnames(USGS.flow.data)

# Rename columns
colnames(USGS.flow.data) <- c("agency_cd", "site_no", "datetime", 
                              "discharge.max", "discharge.max.approval", 
                              "discharge.min", "discharge.min.approval", 
                              "discharge.mean", "discharge.mean.approval", 
                              "gage.height.max", "gage.height.max.approval", 
                              "gage.height.min", "gage.height.min.approval", 
                              "gage.height.mean", "gage.height.mean.approval")

str(USGS.flow.data)
dim(USGS.flow.data)

##head(USGS.flow.data)

class(USGS.flow.data$datetime)

```

## ADJUSTING DATASETS

### Formatting dates

R will often import dates as factors or characters rather than dates. To fix, this we need to tell R that it is looking at dates. We also need to specify the format the dates are in. By default, if you don't provide a format, R will attempt to use %Y-%m-%d or %Y/%m/%d as a default. Note: if you are working collaboratively in an international setting, using a year-month-day format(2019/01/24) in spreadsheets is the least ambiguous of date formats. Make sure to check whether month-day-year or day-month-year is used in an ambiguously formatted spreadsheet.

Formatting of dates in R: 

%d  day as number (0-31)
%m  month (00-12, can be e.g., 01 or 1)
%y  2-digit year
%Y  4-digit year
%a  abbreviated weekday
%A  unabbreviated weekday
%b  abbreviated month
%B  unabbreviated month



In some cases when dates are provided as integers, you may need to provide an origin for your dates.  Beware: the "origin" date for Excel (Windows), Excel (Mac), R, and MATLAB all have different origin dates. Google this if it comes up.

###What is the point of exploring our data?
###To understand data, looking to see if there is anything to transform or

```{r}
help(as.Date)

 ##djust date formatting for today
###Write code for three different date formats. 
 An example is provided to get you started.
 (code must be uncommented)

today <- Sys.Date()
today

 ##1st time
today1<-Sys.Date()
format(today1, format="%Y/%m/%d")

 ##2nd time
today2<-Sys.Date()
format(today2, format="%y/%b/%d")

##3rd time
today3<-Sys.Date()
format(today3, format = "%Y/%b/%d")


#format(today, format = "")
#format(today, format = "")
#format(today, format = "")

##Tell R to change factor to a date, in the format Month/day/year
USGS.flow.data$datetime <- as.Date(USGS.flow.data$datetime, format = "%m/%d/%y") 
```

Note that for every date prior to 1969, R has assigned the date in the 2000s rather than the 1900s. This can be fixed with an `ifelse` statement inside a function. Run through the code below and write what is happening in the comment above each line.

```{r}
# Changes the format of the "datetime" column to a two digit year, a two digit month, and the day as a number
USGS.flow.data$datetime <- format(USGS.flow.data$datetime, "%y%m%d")

#Creating a function that says that if d is greater than 181231, then it will return a 19, if d is not greater than 181231, then it will return a 20.Paste0 takes the two statements that are inside the function (d>181231, "19", "20")

#The 181231 is the last date of 2018. If d is over this value, a 19 is returned, If D<the number, a 20 is returned. 
create.early.dates <- (function(d) {
       paste0(ifelse(d > 181231,"19","20"),d)
       })


#This will input the datetime column into the function we created above for each row of the column
USGS.flow.data$datetime <- create.early.dates(USGS.flow.data$datetime)

#Changes the date time column into a different format to a 4 digit year, a two digit month, and the day as
USGS.flow.data$datetime <- as.Date(USGS.flow.data$datetime, format = "%Y%m%d") 

```

### Removing NAs

Notice in our dataset that our discharge and gage height observations have many NAs, meaning no measurement was recorded for a specific day. In some cases, it might be in our best interest to remove NAs from a dataset. Removing NAs or not will depend on your research question.

```{r}
summary(USGS.flow.data$discharge.mean)
summary(USGS.flow.data$gage.height.mean)
```


Question: What types of research questions might make it favorable to remove NAs from a dataset, and what types of research questions might make it favorable to retain NAs in the dataset?

> Answer: We would remove NA's if you're trying to quantitatively analyze a data set and need the value. It's also important to know if NA means a "below detection limit", so the value isn't 0, but isn't significant enough to be recorded; this is prevalent in water quality data. 

```{r}
#Create new data frame to preserve the complete data set with NA's
USGS.flow.data.complete <- na.omit(USGS.flow.data)   ###It removes the entire rows with any NA's. Naomit operates on rows.

##complete.cases returns which cases are complete; will query for the full rows
dim(USGS.flow.data)
dim(USGS.flow.data.complete)

mean(USGS.flow.data.complete$discharge.mean)
sd(USGS.flow.data.complete$discharge.mean)
summary(USGS.flow.data.complete$discharge.mean)

```

## VISUALIZATION FOR DATA EXPLORATION

Although the `summary()` function is helpful in getting an idea of the spread of values in a numeric dataset, it can be useful to create visual representations of the data to help form hypotheses and direct downstream data analysis. Below is a summary of the useful types of graphs that can be 

Note: each of these approaches utilize the package "ggplot2". We will be covering the syntax of ggplot in a later lesson, but for now you should familiarize yourself with the functionality of what each command is doing.

### Bar Chart (function: geom_bar)

Visualize count data for categorical variables. 

###GGplot2 is a package within the TidyVerse package.  This takes categorical data and fivws us a bar-the height is the number of samples.
Geom_bar works great for categorical variables 
```{r, fig.height = 3, fig.width = 4}
library(ggplot2)
ggplot(USGS.flow.data.complete, aes(x = discharge.mean.approval)) +
  geom_bar()
```

### Histogram (function: geom_histogram)

Visualize distributions of values for continuous numerical variables. What is happening in each line of code? Insert a comment above each line.

```{r, fig.height = 3, fig.width = 4}
#Dscharge_mean is a continuous quantitative variable. Most of the discharge means are very low.
ggplot(USGS.flow.data.complete) +
  geom_histogram(aes(x = discharge.mean))

#It tooks a discharge.mean of 10 units. For every 10 units of the variable, a bin is created. bin 
ggplot(USGS.flow.data.complete) +
  geom_histogram(aes(x = discharge.mean), binwidth = 10)

#Bins just gives a number of bins
ggplot(USGS.flow.data.complete) +
  geom_histogram(aes(x = discharge.mean), bins = 20)

#Add a command telling plot to create a continuous x-axis scale that goes from 0 to 500
ggplot(USGS.flow.data.complete, aes(x = discharge.mean)) +
  geom_histogram(binwidth = 10) + 
  scale_x_continuous(limits = c(0, 500))
  
#gage.height.mean is in meters, and goes from 0-15
ggplot(USGS.flow.data.complete) +
  geom_histogram(aes(x = gage.height.mean))

###You can toggle back and forth between the plots you created using the bluew arrow in the plots window
```
### Frequency line graph (function: geom_freqpoly)

An alternate to a histogram is a frequency polygon graph (distributions of values for continuous numerical variables). Instead of displaying bars, counts of continuous variables are displayed as lines. This is advantageous if you want to display multiple variables or categories of variables at once.

```{r, fig.height = 3, fig.width = 4}
#Create three frequency polygons (lines): one for mean gauge height on any day, one for min gauge height on any day, and one for the max gage height on any day


ggplot(USGS.flow.data.complete) +
  geom_freqpoly(aes(x = gage.height.mean), bins = 50) +
  geom_freqpoly(aes(x = gage.height.min), bins = 50, color = "blue") +
  geom_freqpoly(aes(x = gage.height.max), bins = 50, color = "red") +
  scale_x_continuous(limits = c(0, 10))

###On the peak, most of the counts are around 2.5. 
###For frequency polygons, if you're plotting multiple things, make sure they have the same units or are in the same order of magnitude.

###Here, we're taking measurements in the same column and dividing them up into colors based on the status for approval(A or P), which is a categorical variable. Approval is either A or P.
ggplot(USGS.flow.data.complete) +
  geom_freqpoly(aes(x = gage.height.mean, color = gage.height.mean.approval), bins = 50) +
  scale_x_continuous(limits = c(0, 10)) +
  theme(legend.position = "top")

```
### Box-and-whisker plots (function: geom_boxplot)

A box-and-whisker plot is yet another alternative to histograms (distributions of values for continuous numerical variables). These plots consist of: 

* A box from the 25th to the 75th percentile of the data, called the interquartile range (IQR).

* A bold line inside the box representing the median value of the data. Whether the median is in the center or off to one side of the IQR will give you an idea about the skewness of your data.

* A line outside of the box representing values falling within 1.5 times the IQR. 

* Points representing outliers, values that fall outside 1.5 times the IQR. 

An alternate option is a violin plot, which displays density distributions, somewhat like a hybrid of the box-and-whiskers and the frequency polygon plot.

###fig.height=3, fig.width=4 only changes the dimensions for when you knit
```{r, fig.height = 3, fig.width = 4}
#Plot the values of the mean using a boxplot, and split it up based on whether it has been approved (A) or if it is pending (P)
ggplot(USGS.flow.data.complete) +
  geom_boxplot(aes(x = gage.height.mean.approval, y = gage.height.mean))

#Take gage.height.mean, divide it into units of 1, and then plot the the distribution of discharge associated with the gauge heights. Gage height is the height of the river. The higher the gage height, the higher the discharge. This is for two continuous quantitivate variables.

ggplot(USGS.flow.data.complete) +
  geom_boxplot(aes(x = gage.height.mean, y = discharge.mean, group = cut_width(gage.height.mean, 1)))

#The geom-violin draws quantiles of 0.25, 0.5, and 0.75. Shows that the greater the amount of samples, the wider the violin. The outliers are the skinny lines on the graph.
ggplot(USGS.flow.data.complete) +
  geom_violin(aes(x = gage.height.mean.approval, y = gage.height.mean), draw_quantiles = c(0.25, 0.5, 0.75))
```

Question: what are the pros and cons of each type of frequency graph (histogram, frequency polygon, box-and whisker, violin)?

> Answer: 


### Scatterplot (function: geom_point)
Visualize relationships between continuous numerical variables.

```{r, fig.height = 3, fig.width = 4}
ggplot(USGS.flow.data.complete) +
  geom_point(aes(x = discharge.mean, y = gage.height.mean))

```

## ENDING DISCUSSION

How can multiple options for data exploration inform our understanding of our data? What did you learn about the USGS discharge dataset today?

> ANSWER: 

This passage from R for Data Science sums up some of the questions we should ask ourselves when initially exploring a dataset. "Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:

"Could this pattern be due to coincidence (i.e. random chance)?

"How can you describe the relationship implied by the pattern?

"How strong is the relationship implied by the pattern?

"What other variables might affect the relationship?

"Does the relationship change if you look at individual subgroups of the data?"

Do you see any patterns in the USGS data for the Eno River? What might be responsible for those patterns and/or relationships?

> ANSWER: 


```{r}

```
