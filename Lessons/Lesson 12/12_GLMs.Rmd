---
title: "11: Generalized Linear Models"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2019"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: inline
---

## LESSON OBJECTIVES
1. Describe the components of the generalized linear model (GLM)
2. Apply special cases of the GLM to real datasets
3. Interpret and report the results of GLMs in publication-style formats

## SET UP YOUR DATA ANALYSIS SESSION

```{r, message = FALSE, warning = FALSE}
getwd()
library(tidyverse)
setwd("~/Desktop/Environmental Data Analytics/Environmental_Data_Analytics/Data/Processed")
PeterPaul.chem.nutrients <- read.csv("NTL-LTER_Lake_Chemistry_Nutrients_PeterPaul_Processed.csv")

# Set date to date format
PeterPaul.chem.nutrients$sampledate <- as.Date(PeterPaul.chem.nutrients$sampledate, 
                                               format = "%Y-%m-%d")

gabytheme <- theme_bw(base_size = 14) + 
  theme(plot.title=element_text(face="bold", size="16", color="tomato2", hjust=0.5),
        axis.title=element_text(face="bold.italic", size=11, color="black"),
axis.text = element_text(face="bold", size=10, color = "black"), 
panel.background=element_rect(fill="lightgray", color="darkblue"), 
panel.border = element_rect(color = "black", size = 2),
legend.position = "top", legend.background = element_rect(fill="white", color="black"),
            legend.key = element_rect(fill="transparent", color="NA"))

```


## SIMPLE AND MULTIPLE LINEAR REGRESSION
The linear regression, like the t-test and ANOVA, is a special case of the **generalized linear model** (GLM). A linear regression is comprised of a continuous response variable, plus a combination of 1+ continuous response variables (plus the error term). The deterministic portion of the equation describes the response variable as lying on a straight line, with an intercept and a slope term. The equation is thus a typical algebraic expression: 
$$ y = \alpha + \beta*x + \epsilon $$

The goal for the linear regression is to find a **line of best fit**, which is the line drawn through the bivariate space that minimizes the total distance of points from the line. This is also called a "least squares" regression. The remainder of the variance not explained by the model is called the **residual error.** 

This line will have the least error is the least sqaures regression

The linear regression will test the null hypotheses that

1. The intercept (alpha) is equal to zero.
2. The slope (beta) is equal to zero

Whether or not we care about the result of each of these tested hypotheses will depend on our research question. Sometimes, the test for the intercept will be of interest, and sometimes it will not.

Important components of the linear regression are the correlation and the R-squared value. The **correlation**, or r, is a number between -1 and 1, describing the relationship between the variables. Correlations close to -1 represent strong negative correlations, correlations close to zero represent weak correlations, and correlations close to 1 represent strong positive correlations. The **R-squared value** is the correlation squared, becoming a number between 0 and 1. The R-squared value describes the percent of variance accounted for by the explanatory variables. 

### Simple Linear Regression
For the NTL-LTER dataset, can we predict irradiance (light level) from depth?
```{r}
irradiance.regression <- lm(PeterPaul.chem.nutrients$irradianceWater ~ PeterPaul.chem.nutrients$depth)
# another way to format the lm function (y variable on left of tilda, x variable on right of tilda)

```

```{r}
irradiance.regression <- lm(data = PeterPaul.chem.nutrients, irradianceWater ~ depth)
summary(irradiance.regression)
```
#Adjusted r-sqaured penalizes you for adding more explanatory variables. Because we're only using 1 explanatory variable, our two R-Squareds are the same. This gives you idea of how much explanatory power you have.

# Correlation
```{r}
cor.test(PeterPaul.chem.nutrients$irradianceWater, PeterPaul.chem.nutrients$depth)
```



Question: How would you report the results of this test (overall findings and report of statistical output)?

> 
irradiance=487-95.89(depth)+E (this could be added in figure as a geom_text)
For each meter of depth we go up, irradiance will decrease by 96 units
Irradiance decreases by 96 units for each one meter increase in depth. Irradiance is negatively correlated with depth.
ANSWER:(Linear Regression; p<0.0001, df=15,449, Rsqaured=0.3091)

###We only report the F-statistic for an anova


So, we see there is a significant negative correlation between irradiance and depth (lower light levels at greater depths), and that this model explains about 31 % of the total variance in irradiance. Let's visualize this relationship and the model itself. 

An exploratory option to visualize the model fit is to use the function `plot`. This function will return four graphs, which are intended only for checking the fit of the model and not for communicating results. The plots that are returned are: 

1. **Residuals vs. Fitted.** The value predicted by the line of best fit is the fitted value, and the residual is the distance of that actual value from the predicted value. By definition, there will be a balance of positive and negative residuals. Watch for drastic asymmetry from side to side or a marked departure from zero for the red line - these are signs of a poor model fit.

2. **Normal Q-Q.** The points should fall close to the 1:1 line. We often see departures from 1:1 at the high and low ends of the dataset, which could be outliers. 

3. **Scale-Location.** Similar to the residuals vs. fitted graph, this will graph the squared standardized residuals by the fitted values. 

4. **Residuals vs. Leverage.** This graph will display potential outliers. The values that fall outside the dashed red lines (Cook's distance) are outliers for the model. Watch for drastic departures of the solid red line from horizontal - this is a sign of a poor model fit.

```{r, fig.height = 3, fig.width = 4}
par(mfrow=c(2,2))
plot(irradiance.regression)
```

The option best suited for communicating findings is to plot the explanatory and response variables as a scatterplot. 

###remove the crazy outlier by adding ylim(0,2000)
###Log trasnform irradiance because it looks like our data is non-linear
```{r, fig.height = 3, fig.width = 4}

# Plot the regression
irradiancebydepth <- 
  ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = irradianceWater)) +
  ylim(0, 2000) +
  geom_point() 
print(irradiancebydepth) 
```

##Given the distribution of irradiance values, we don't have a linear relationship between x and y in this case. Let's try log-transforming the irradiance values.
###We can't log-transform values of zero. We have three values=0 of irradianceWater Variable, so weremove it because we only have three observations of 0. 

#Log Trasnform Y-value of IrradianceWater
```{r, fig.height = 3, fig.width = 4}
PeterPaul.chem.nutrients <- filter(PeterPaul.chem.nutrients, irradianceWater != 0)
irradiance.regression2 <- lm(data = PeterPaul.chem.nutrients, log(irradianceWater) ~ depth)
summary(irradiance.regression2)
```
log(Irradiance)=6.2-0.74(depth) + E
This model explains more of the variance-better model


#Plot Residuals
```{r}
par(mfrow=c(2,2))
plot(irradiance.regression2)




```
###Residuals look normal


# Add a line and standard error for the linear regression
```{r}
irradiancebydepth2 <- 
  ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = irradianceWater)) +
  geom_smooth(method = "lm") +
  scale_y_log10() +  ###this puts log-transformed breaks in the y-scale
  geom_point() +
  gabytheme
print(irradiancebydepth2) 

```


# SE can also be removed
```{r}
irradiancebydepth2 <- 
    ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = irradianceWater)) +
    geom_point(alpha=0.5) +  #alpha changes the transparency of the points
    scale_y_log10() +
  scale_x_continuous(breaks=c(0,2,4,6,8,10,12))+ xlim(0,16) +
    geom_smooth(method = 'lm', se = TRUE, color = "tomato") +
  gabytheme+
  labs(title="The Effect of Depth on the Irradiance of Water", x="Depth(m)", y="Water Irradiance(units)")
  ###geom_text(=)   ###Add line of eqaution
print(irradiancebydepth2)
```
#SE=FALSE means that there is no confidence inteval 



### Non-parametric equivalent: Spearman's Rho
As with the t-test and ANOVA, there is a nonparametric variant to the linear regression. The **Spearman's rho** test has the advantage of not depending on the normal distribution, but this test is not as robust as the linear regression.


```{r}
cor.test(PeterPaul.chem.nutrients$irradianceWater, PeterPaul.chem.nutrients$depth, method = "spearman", exact = FALSE)
```




# Multiple Regression
It is possible, and often useful, to consider multiple continuous explanatory variables at a time in a linear regression. For example, total phosphorus concentration could be dependent on depth and dissolved oxygen concentration: 


```{r, fig.height = 3, fig.width = 4}
TPregression <- lm(data = PeterPaul.chem.nutrients, tp_ug ~ depth + dissolvedOxygen)
summary(TPregression)
```
Total Phosphorus(ug/L)=6+ 1.5(depth in meters) + 0.94(dissolvedOxygen)



###Shows us that our linear model isn't explaining this data 

```{r}
TPplot <- ggplot(PeterPaul.chem.nutrients, 
                 aes(x = dissolvedOxygen, y = tp_ug, color = depth)) +
  geom_point() +
  xlim(0, 20)
print(TPplot)
```




# Correlation Plots
We can also make exploratory plots of several continuous data points to determine possible relationships, as well as covariance among explanatory variables. 

```{r, fig.height = 3, fig.width = 4}
install.packages("corrplot")
library(corrplot)
PeterPaulnutrients <- 
  PeterPaul.chem.nutrients %>%
  select(tn_ug:po4) %>%  ##selected only the columns that pertain to nutrients
  na.omit()     ###omitted the NA's, This omits NA across total data set, which is why she selected the columns of nutrients first, so it wouldn't affect other data
PeterPaulCorr <- cor(PeterPaulnutrients)
corrplot(PeterPaulCorr, method = "ellipse")
corrplot.mixed(PeterPaulCorr, upper = "ellipse")  ###OR USE GGPAIRS
```
### AIC to select variables



However, it is possible to over-parameterize a linear model. Adding additional explanatory variables takes away degrees of freedom, and if explanatory variables co-vary the interpretation can become overly complicated. Remember, an ideal statistical model balances simplicity and explanatory power! To help with this tradeoff, we can use the **Akaike's Information Criterion (AIC)** to compute a stepwise regression that either adds explanatory variables from the bottom up or removes explanatory variables from a full set of suggested options. The smaller the AIC value, the better. 

Let's say we want to know which explanatory variables will allow us to best predict total phosphorus concentrations. Potential explanatory variables from the dataset could include depth, dissolved oxygen, temperature, PAR, total N concentration, and phosphate concentration.

```{r}
PeterPaul.naomit <- na.omit(PeterPaul.chem.nutrients)  ##omit NA's from data set-we want to put in only complete info. Because this only gives us like 242 variables, we need to ensure we maximize our explanatory power of the model
```


#Predict total phosphorus based on depth and dissolved oxygen, and temperature, total nitrogen, and phosphate
```{r}
TPAIC <- lm(data = PeterPaul.naomit, tp_ug ~ depth + dissolvedOxygen + 
              temperature_C + tn_ug + po4)

```



It tests all of the potential subrtratctions it makes for the model.

```{r}
step(TPAIC)  ###Step function 
TPmodel <- lm(data = PeterPaul.naomit, tp_ug ~ temperature_C + tn_ug)
summary(TPmodel)

```


#Run Best Fit model on its own
```{r}
TPmodel <- lm(data = PeterPaul.naomit, tp_ug ~ temperature_C + tn_ug)
summary(TPmodel)
```
###With one unit increase in temeprature, we decrease temperature by 0.46 units, and increase total nitrogen by 0.027. 


## ANCOVA
Analysis of Covariance consists of a prediction of a continuous response variable by both continuous and categorical explanatory variables. We set this up in R with the `lm` function, just like prior applications in this lesson. 

Let's say we wanted to predict total nitrogen concentrations by depth and by lake, similarly to what we did with a two-way ANOVA for depth ID and lake. 


###Adds mulltiple levels of alphas
###ANCOVAS are a combination of ANOVA and a linear regression

```{r, fig.height = 3, fig.width = 4}
# main effects
TNancova.main <- lm(data = PeterPaul.chem.nutrients, tn_ug ~ lakename + depth)
summary(TNancova.main)
```
###Total Nitrogen=353(PaulLakeIntercept at depth of 0) +135(Peter Lake) -9.7(Depth) + E
###However, depth is not a significant predictor based on p-value
###Rsquared is abysmal-explains almost no variance 


# Interaction effects
```{r}

TNancova.interaction <- lm(data = PeterPaul.chem.nutrients, tn_ug ~ lakename * depth)
summary(TNancova.interaction)
```
##Total Nitrogen=325(Paul Lake at 0 meters depth)+185(Peter Lake)+20(Paul Lake:Depth-48(Peter Lake:Depth) + E

Better model than above

```{r}
TNplot <- ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = tn_ug, color = lakename)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  xlim(0, 10) +gabytheme
print(TNplot)
```
###Anocova makes different slopes and intercepts based on the categorical variables (Paul Lake and Peter Lake)


