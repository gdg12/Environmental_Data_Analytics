---
title: "8: Data Wrangling"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2019"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: inline
---

## LESSON OBJECTIVES
1. Describe the usefulness of data wrangling and its place in the data pipeline
2. Wrangle datasets with dplyr functions
3. Apply data wrangling skills to a real-world example dataset

## SET UP YOUR DATA ANALYSIS SESSION

```{r}

setwd("~/Desktop/Environmental Data Analytics/Environmental_Data_Analytics/Data/Raw")
library(tidyverse)
NTL.phys.data.PeterPaul <- read.csv("NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv")
NTL.nutrient.data <- read.csv("NTL-LTER_Lake_Nutrients_Raw.csv")
```

## REVIEW OF BASIC DATA EXPLORATION AND WRANGLING
```{r}
# Data summaries for physical data
head(NTL.phys.data.PeterPaul)

```




```{r}

class(NTL.phys.data.PeterPaul$sampledate)
```

```{r}
colnames(NTL.phys.data.PeterPaul)
```

```{r}
dim(NTL.phys.data.PeterPaul)
```

```{r}
summary(NTL.phys.data.PeterPaul$comments)
```



# Format sampledate as date
```{r}
NTL.phys.data.PeterPaul$sampledate <- as.Date(NTL.phys.data.PeterPaul$sampledate, format = "%m/%d/%y")
```


# Select Peter and Paul Lakes from the nutrient dataset. "Filter" selects rows, "Select" chooses columns
```{r}
NTL.nutrient.data.PeterPaul <- filter(NTL.nutrient.data, lakename == "Paul Lake" | lakename == "Peter Lake")
```
###We use the vertical line because none of them are both lakes-it's either one lake or the other. 


# Data summaries for nutrient data
```{r}

head(NTL.nutrient.data.PeterPaul)
```


```{r}
colnames(NTL.nutrient.data.PeterPaul)
```
###tn_ug is Total nitrogen in micrograms/liter, tp_ug is total phosphorus in micrograms/liter


```{r}
dim(NTL.nutrient.data.PeterPaul)
```

```{r}
summary(NTL.nutrient.data.PeterPaul$comments)
```
###2770 samples that are blank, 0 are sample missing, 0 samples are too high
###These comments were made for other lakes in the original dataset, but we filtered out those other lakes, which is why we have 0
###The comment variable is a factor. For numeric values empty cells will fill in with NAs, but for character or factor values, the cell will remain blank.

```{r}
summary(NTL.nutrient.data.PeterPaul$lakename)
```

```{r}
NTL.nutrient.data.PeterPaul$sampledate <- as.Date(NTL.nutrient.data.PeterPaul$sampledate, format = "%m/%d/%y")
```

###Check to see if sample date is a date
```{r}
class(NTL.nutrient.data.PeterPaul$sampledate)
```



# Save processed nutrient file
Rownames=false means that the row names are the first columns 1,2,3,4...We don't want R to save that column
```{r}
setwd("~/Desktop/Environmental Data Analytics/Environmental_Data_Analytics/Data/Processed")
write.csv(NTL.nutrient.data.PeterPaul, row.names = FALSE, file = "NTL-LTER_Lake_Nutrients_PeterPaul_Processed.csv")
```



# Remove columns that are not of interest for analysis
```{r}
NTL.phys.data.PeterPaul.skinny <- select(NTL.phys.data.PeterPaul, 
                                         lakename, year4, sampledate:irradianceDeck)

###We want to get rid of depth_id and comments columns

  
```
###Select function specifies which columns we want the data frame to keep
  
  
  
```{r}
NTL.nutrient.data.PeterPaul.skinny <- select(NTL.nutrient.data.PeterPaul, 
                                             lakename, year4, sampledate, depth:po4)
```
  



## TIDY DATASETS

For most situations, data analysis works best when you have organized your data into a tidy dataset. A tidy dataset is defined as: 

* Each variable is a column
* Each row is an observation (e.g., sampling event from a specific date and/or location)
* Each value is in its own cell

However, there may be situations where we want to reshape our dataset, for example if we want to facet numerical data points by measurement type (more on this in the data visualization unit). We can program this reshaping in a few short lines of code using the package `tidyr`, which is conveniently included in the `tidyverse` package. 


```{r}
dim(NTL.nutrient.data.PeterPaul.skinny)
```

```{r}
dim(NTL.phys.data.PeterPaul.skinny)
```

```{r}
# Gather nutrient data into one column. Gather takes the
###Gather function  is part of TidyR package. Gather skinny data set for nutrient data with new columns (nutrient and concnetration), then tell it which columns to gather L tn_ug:po4

###We would gather data sets to, for ex, split the data set based on
NTL.nutrient.data.PeterPaul.gathered <- gather(NTL.nutrient.data.PeterPaul.skinny, "nutrient", "concentration", tn_ug:po4)
dim(NTL.nutrient.data.PeterPaul.gathered)
```



###In nutrient data, a lot of days don't have nitrate measurements-->we can just remove those rows entirely. Subset data frame by the condition of removing NA's associated with concentration
###Gather takes the maximum number of samples and fills in missing cells with NAs
```{r}
NTL.nutrient.data.PeterPaul.gathered <- subset(NTL.nutrient.data.PeterPaul.gathered, !is.na(concentration))
```


###Now if you look at counts, the data set has been filtered. Gathered takes the max number of observations in original data set and whittle them down 
```{r}
count(NTL.nutrient.data.PeterPaul.gathered, nutrient)
```



```{r}
setwd("~/Desktop/Environmental Data Analytics/Environmental_Data_Analytics/Data/Processed")
write.csv(NTL.nutrient.data.PeterPaul.gathered, row.names = FALSE, 
          file ="NTL-LTER_Lake_Nutrients_PeterPaulGathered_Processed.csv")
```





###Spread is the opposite of "gather" function. Say we want each nutrient to have a different column in data set. 
### Spread nutrient data into separate columns. The concnetration is the actual number associsted with nutrients
```{r}
NTL.nutrient.data.PeterPaul.spread <- spread(NTL.nutrient.data.PeterPaul.gathered, nutrient, concentration)
```






# Split components of cells into multiple columns
# Opposite of 'separate' is 'unite'

###Ex, we have a column in NTL.nutrient.data.PeterPaul.dates with complete date, but we want to separate this into three separate columns with year, month, day, It doesn't keep the original date column though. 
```{r}
NTL.nutrient.data.PeterPaul.dates <- separate(NTL.nutrient.data.PeterPaul.skinny, sampledate, c("Y", "m", "d"))
```


```{r}
class(NTL.nutrient.data.PeterPaul.dates$Y)
```





## JOINING MULTIPLE DATASETS
In many cases, we will want to combine datasets into one dataset. If all column names match, the data frames can be combined with the `rbind` function. If some column names match and some column names don't match, we can combine the data frames using a "join" function according to common conditions that exist in the matching columns. We will demonstrate this with the NTL-LTER physical and nutrient datasets, where we have specific instances when physical and nutrient data were collected on the same date, at the same lake, and at the same depth. 

In dplyr, there are several types of join functions: 

* `inner_join`: return rows in x where there are matching values in y, and all columns in x and y (mutating join).
-->Must specify which dataset is first(the order)


* `semi_join`: return all rows from x where there are matching values in  y, keeping just columns from x (filtering join).-->eliminate rows in data set based on certain conditions )
-->Filtering X according to conditions in Y

* `left_join`: return all rows from x, and all columns from x and y (mutating join)
-->Takes nuimber of rows in X, and if any observations in Y match, takes the observations at end of data frame.


* `anti_join`: return all rows from x where there are *not* matching values in y, keeping just columns from x (filtering join).
-->Say we want to grab dates where only temp and DO were taken


* `full_join`: return all rows and all columns from x and y. Returns NA for missing values (mutating join).
-->Takes all rows and colummn from X and Y and mathc them up according to whether the observations match. Joining factors with different levels and coercing them to a character vector


#Let's say we want to generate a new dataset that contains all possible physical and chemical data for Peter and Paul Lakes. In this case, we want to do a full join.
```{r}

NTL.phys.nutrient.data.PeterPaul <- full_join(NTL.phys.data.PeterPaul.skinny, NTL.nutrient.data.PeterPaul.skinny)
NTL.phys.nutrient.data.PeterPaul
write.csv(NTL.phys.nutrient.data.PeterPaul, row.names = FALSE, file="NTL-LTER_Lake_Chemistry_Nutrients_PeterPaul_Processed.csv")
```





## LUBRIDATE

A package that makes coercing dates much easier is `lubridate`. A guide to the package can be found at https://lubridate.tidyverse.org/. The cheat sheet within that web page is excellent too. This package can do many things (hint: look into this package if you are having unique date-type issues), but today we will be using two of its functions for our NTL dataset. 

```{r}
install.packages(lubridate)
library(lubridate)

# add a month column to the dataset
NTL.phys.nutrient.data.PeterPaul <- mutate(NTL.phys.nutrient.data.PeterPaul, month = month(sampledate)) 
NTL.phys.nutrient.data.PeterPaul
```
###Remember mutate adds columns to the end of the df. Now, a month column has been added to the end of the data frame


```{r}
# reorder columns to put month with the rest of the date variables
NTL.phys.nutrient.data.PeterPaul <- select(NTL.phys.nutrient.data.PeterPaul, lakename, year4, month, sampledate:po4)
NTL.phys.nutrient.data.PeterPaul
```
##Select all of the columns, but in a different order than before. Now month is right after the year column. 


# find out the start and end dates of the dataset
```{r}
interval(NTL.phys.nutrient.data.PeterPaul$sampledate[1], NTL.phys.nutrient.data.PeterPaul$sampledate[23372])

###While this is easy to use, it is not reproducible because the sample dates could change-more sample dates could be added and this code wouldn't work anymore


interval(first(NTL.phys.nutrient.data.PeterPaul$sampledate), last(NTL.phys.nutrient.data.PeterPaul$sampledate))
###This code gives us the same output but it's reproducible and will update for new sampling dates, or if sampling dates are out of order. 
```
###Warning message: Takes the normal date function in the base R, and now lubridate is taking over the date function.



## SPLIT-APPLY-COMBINE
Group our data sets based on certain conditions, then run operations on the groups.

dplyr functionality, combined with the pipes operator, allows us to split datasets according to groupings (function: `group_by`), then run operations on those groupings and return the output of those operations. There is a lot of flexibility in this approach, but we will illustrate just one example today.

```{r}
NTL.PeterPaul.summaries <- 
  NTL.phys.nutrient.data.PeterPaul %>%   ###pipe indicates then
  filter(depth == 0) %>%   ###take only data with the surface depths=0
  group_by(lakename) %>%  ###group by lake names
  filter(!is.na(temperature_C) & !is.na(tn_ug) & !is.na(tp_ug)) %>%   ###get rid of NA's in temperature, tn_ug, and tp_ug columns. It only takes observations where we have measurements for ALL temperature, tn_ug, and tp_ug. 
  
  ##We should get two means for temperature because we told dplyr to group by lake name
  summarize(meantemp = mean(temperature_C), 
            sdtemp = sd(temperature_C), 
            meanTN = mean(tn_ug), 
            sdTN = sd(tn_ug), 
            meanTP = mean(tp_ug), 
            sdTP = sd(tp_ug))

NTL.PeterPaul.summaries
```
###This is helpful for barcharts


```{r}
write.csv(NTL.PeterPaul.summaries, row.names=FALSE, file="NTL-LTER_Lake_Summaries_PeterPaul_Processed.csv")
```

## ALTERNATIVE METHODS FOR DATA WRANGLING

If you want to iteratively perform operations on your data, there exist several options. We have demonstrated the pipe as one option. Additional options include the `apply` function (https://www.rdocumentation.org/packages/base/versions/3.5.2/topics/apply) and `for` loops (https://swcarpentry.github.io/r-novice-inflammation/15-supp-loops-in-depth/). These options are good options as well (again, multiple ways to get to the same outcome). A word of caution: loops are slow. This may not make a difference for small datasets, but small time additions will make a difference with large datasets.